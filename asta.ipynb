{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CUDA/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {torch.cuda.get_device_name(device)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataHandler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# DataHandler Class\n",
    "# ---------------------------\n",
    "class DataHandler:\n",
    "    def __init__(self):\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        self.scaler = MinMaxScaler()\n",
    "    \n",
    "    def fetch_data(self, symbol, start_date, end_date):\n",
    "        print(f\"Fetching data for {symbol} from {start_date} to {end_date}...\")\n",
    "        df = yf.download(symbol, start=start_date, end=end_date)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"No data found for {symbol}\")\n",
    "        return self.prepare_data(df)\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        df = df.copy()\n",
    "        # Preserve the original closing price for portfolio simulation\n",
    "        df['CloseOrig'] = df['Close'].astype(float)\n",
    "        \n",
    "        # Convert columns explicitly to one-dimensional arrays (to avoid TA-Lib errors)\n",
    "        close_prices = np.array(df['Close'], dtype=np.float64).flatten()\n",
    "        volume = np.array(df['Volume'], dtype=np.float64).flatten()\n",
    "        \n",
    "        # Compute technical indicators\n",
    "        df['Returns'] = df['Close'].pct_change()\n",
    "        df['SMA_20'] = talib.SMA(close_prices, timeperiod=20)\n",
    "        df['EMA_20'] = talib.EMA(close_prices, timeperiod=20)\n",
    "        df['RSI'] = talib.RSI(close_prices, timeperiod=14)\n",
    "        macd, signal, _ = talib.MACD(close_prices)\n",
    "        df['MACD'] = macd\n",
    "        df['MACD_signal'] = signal\n",
    "        bb_upper, bb_middle, bb_lower = talib.BBANDS(close_prices)\n",
    "        df['BB_upper'] = bb_upper\n",
    "        df['BB_middle'] = bb_middle\n",
    "        df['BB_lower'] = bb_lower\n",
    "        df['OBV'] = talib.OBV(close_prices, volume)\n",
    "        df['MOM'] = talib.MOM(close_prices, timeperiod=14)\n",
    "        \n",
    "        # List of features to normalize (do not scale CloseOrig)\n",
    "        features = ['Close', 'Returns', 'SMA_20', 'EMA_20', 'RSI', \n",
    "                    'MACD', 'MACD_signal', 'BB_upper', 'BB_middle', \n",
    "                    'BB_lower', 'OBV', 'MOM']\n",
    "        df[features] = df[features].fillna(method='ffill').fillna(method='bfill')\n",
    "        df[features] = self.scaler.fit_transform(df[features])\n",
    "        df = df.dropna()\n",
    "        print(f\"Prepared data shape for {df.index[-1]} rows and {len(df.columns)} columns.\")\n",
    "        return df\n",
    "    \n",
    "    def fetch_multiple_data(self, symbols, start_date, end_date):\n",
    "        data_dict = {}\n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                data = self.fetch_data(symbol, start_date, end_date)\n",
    "                data_dict[symbol] = data\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {symbol}: {e}\")\n",
    "        return data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading Env Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Trading Environment Class\n",
    "# ---------------------------\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=100000):\n",
    "        if data is None or data.empty:\n",
    "            raise ValueError(\"Data cannot be empty\")\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        # Features used as state (these are the normalized technical indicators)\n",
    "        self.features = ['Close', 'Returns', 'SMA_20', 'EMA_20', 'RSI',\n",
    "                         'MACD', 'MACD_signal', 'BB_upper', 'BB_middle',\n",
    "                         'BB_lower', 'OBV', 'MOM']\n",
    "        # \"CloseOrig\" is used for actual portfolio value calculation.\n",
    "        if 'CloseOrig' not in self.data.columns:\n",
    "            raise ValueError(\"Missing 'CloseOrig' column in data.\")\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0.0  # shares held\n",
    "        self.current_step = 0\n",
    "        self.portfolio_value_history = [self.initial_balance]\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        return self.data.iloc[self.current_step][self.features].values\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Retrieve actual price from the unscaled \"CloseOrig\"\n",
    "        current_price = float(self.data.iloc[self.current_step]['CloseOrig'])\n",
    "        prev_value = self.portfolio_value_history[-1]\n",
    "        # Action space (5 discrete actions):\n",
    "        # 0: Sell all, 1: Sell half, 2: Hold, 3: Buy with 50% cash, 4: Buy with full cash.\n",
    "        if current_price > 0:\n",
    "            if action == 0 and self.position > 0:\n",
    "                # Sell all\n",
    "                self.balance += current_price * self.position\n",
    "                self.position = 0.0\n",
    "            elif action == 1 and self.position > 0:\n",
    "                # Sell half\n",
    "                shares_to_sell = self.position * 0.5\n",
    "                self.balance += current_price * shares_to_sell\n",
    "                self.position -= shares_to_sell\n",
    "            elif action == 3 and self.balance > 0:\n",
    "                # Buy with 50% cash\n",
    "                cash_to_use = self.balance * 0.5\n",
    "                shares_to_buy = cash_to_use / current_price\n",
    "                self.position += shares_to_buy\n",
    "                self.balance -= cash_to_use\n",
    "            elif action == 4 and self.balance > 0:\n",
    "                # Buy with full cash\n",
    "                shares_to_buy = self.balance / current_price\n",
    "                self.position += shares_to_buy\n",
    "                self.balance = 0\n",
    "            # Action 2 (Hold) does nothing.\n",
    "        new_value = self.balance + (self.position * current_price)\n",
    "        self.portfolio_value_history.append(new_value)\n",
    "        reward = (new_value - prev_value) / prev_value if prev_value > 0 else 0\n",
    "        self.current_step += 1\n",
    "        done = (self.current_step >= len(self.data) - 1)\n",
    "        next_state = self._get_state() if not done else np.zeros(len(self.features))\n",
    "        return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# DQNAgent Class\n",
    "# ---------------------------\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0  # initial exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action_values = self(state_tensor)\n",
    "            return torch.argmax(action_values).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = torch.FloatTensor([m[0] for m in minibatch]).to(device)\n",
    "        actions = torch.LongTensor([m[1] for m in minibatch]).to(device)\n",
    "        rewards = torch.FloatTensor([m[2] for m in minibatch]).to(device)\n",
    "        next_states = torch.FloatTensor([m[3] for m in minibatch]).to(device)\n",
    "        dones = torch.FloatTensor([m[4] for m in minibatch]).to(device)\n",
    "        current_q = self(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        with torch.no_grad():\n",
    "            next_q = self(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        loss = self.criterion(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main file Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Aggregate Data Function\n",
    "# ---------------------------\n",
    "def aggregate_data(data_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary of DataFrames (keyed by symbol), find the common date\n",
    "    index and average the features.\n",
    "    \"\"\"\n",
    "    common_index = None\n",
    "    for df in data_dict.values():\n",
    "        if common_index is None:\n",
    "            common_index = df.index\n",
    "        else:\n",
    "            common_index = common_index.intersection(df.index)\n",
    "    reindexed = []\n",
    "    for symbol, df in data_dict.items():\n",
    "        reindexed.append(df.loc[common_index])\n",
    "    concatenated = pd.concat(reindexed, axis=1, keys=data_dict.keys())\n",
    "    # Average features across symbols; this applies to every column including CloseOrig.\n",
    "    aggregated = concatenated.groupby(axis=1, level=1).mean()\n",
    "    return aggregated\n",
    "\n",
    "# ---------------------------\n",
    "# Training Function\n",
    "# ---------------------------\n",
    "def train_agent(agent, training_datasets, episodes):\n",
    "    training_losses = []\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    # Training proceeds across episodes.\n",
    "    # In each episode we randomly choose one aggregated dataset (i.e. one country) \n",
    "    # and simulate a complete trading episode over its available historical period.\n",
    "    for episode in tqdm(range(episodes), desc=\"Training Episodes\"):\n",
    "        # Randomly choose one country's aggregated data\n",
    "        country, data = random.choice(list(training_datasets.items()))\n",
    "        env = TradingEnvironment(data, initial_balance=100000)\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            loss = agent.replay()\n",
    "            if loss:\n",
    "                episode_losses.append(loss)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                avg_loss = np.mean(episode_losses) if episode_losses else 0\n",
    "                training_losses.append(avg_loss)\n",
    "                if total_reward > best_reward:\n",
    "                    best_reward = total_reward\n",
    "                    torch.save(agent.state_dict(), 'final_model.pth')\n",
    "                print(f\"\\nEpisode {episode+1}/{episodes} | Country: {country} | Total Reward: {total_reward:.2f} | Avg Loss: {avg_loss:.4f} | Epsilon: {agent.epsilon:.4f} | Final Portfolio: ${env.portfolio_value_history[-1]:.2f}\")\n",
    "                break\n",
    "    return training_losses\n",
    "\n",
    "# ---------------------------\n",
    "# Main Routine for Training on Multiple Indexes\n",
    "# ---------------------------\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Define training period with a larger dataset\n",
    "        TRAIN_START_DATE = '1995-01-01'\n",
    "        TRAIN_END_DATE   = '2024-01-01'\n",
    "        EPISODES = 100  # adjust number of episodes as needed\n",
    "        \n",
    "        # Dictionary of indexes and their top 10 companies (tickers)\n",
    "        index_companies = {\n",
    "            \"USA\": ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'GOOGL', 'META', 'TSLA', 'BRK-B', 'UNH', 'XOM'],\n",
    "            \"India\": ['RELIANCE.NS', 'TCS.NS', 'HDFCBANK.NS', 'INFY.NS', 'ICICIBANK.NS', 'KOTAKBANK.NS', 'LT.NS', 'AXISBANK.NS', 'ITC.NS', 'HINDUNILVR.NS'],\n",
    "            \"Japan\": ['7203.T', '6758.T', '9984.T', '8306.T', '6902.T', '9432.T', '7267.T', '7974.T', '6501.T', '8801.T'],\n",
    "            \"UK\": ['HSBA.L', 'BP.L', 'VOD.L', 'GSK.L', 'RIO.L', 'BT-A.L', 'ULVR.L', 'DGE.L', 'AZN.L', 'BATS.L'],\n",
    "            \"France\": ['OR.PA', 'MC.PA', 'SAN.PA', 'AI.PA', 'BNP.PA', 'DG.PA', 'EN.PA', 'RI.PA', 'KER.PA', 'SU.PA']\n",
    "        }\n",
    "        \n",
    "        # ... rest of your training code remains unchanged ...\n",
    "        \n",
    "        data_handler = DataHandler()\n",
    "        training_datasets = {}\n",
    "        \n",
    "        # For each country/index, fetch data for its top companies and aggregate.\n",
    "        for country, tickers in index_companies.items():\n",
    "            print(f\"\\nProcessing {country} data:\")\n",
    "            company_data = data_handler.fetch_multiple_data(tickers, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "            if company_data:\n",
    "                agg_data = aggregate_data(company_data)\n",
    "                training_datasets[country] = agg_data\n",
    "            else:\n",
    "                print(f\"No valid data fetched for {country}.\")\n",
    "        \n",
    "        if not training_datasets:\n",
    "            raise ValueError(\"No aggregated training data available from any index!\")\n",
    "        \n",
    "        # All training datasets must have the expected technical indicator columns.\n",
    "        # We assume each aggregated DataFrame contains at least the following columns:\n",
    "        # ['Close', 'Returns', 'SMA_20', 'EMA_20', 'RSI', 'MACD', 'MACD_signal', 'BB_upper', 'BB_middle', 'BB_lower', 'OBV', 'MOM', 'CloseOrig']\n",
    "        \n",
    "        # Initialize the training environment parameters are set within the episode loop.\n",
    "        state_size = 12  # corresponds to the feature vector (excluding CloseOrig)\n",
    "        action_size = 5  # five possible actions\n",
    "        \n",
    "        # Initialize the DQN agent\n",
    "        agent = DQNAgent(state_size, action_size)\n",
    "        \n",
    "        print(\"\\nStarting training on multiple indexes...\")\n",
    "        training_losses = train_agent(agent, training_datasets, EPISODES)\n",
    "        \n",
    "        print(\"\\nTraining Completed. Best model saved as final_model.pth\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the device for computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------\n",
    "# DataHandler: Downloads and Prepares Test Data\n",
    "# -------------------------\n",
    "class DataHandler:\n",
    "    def __init__(self):\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def fetch_data(self, symbol, start_date, end_date):\n",
    "        print(f\"\\nFetching data for {symbol} from {start_date} to {end_date}...\")\n",
    "        df = yf.download(symbol, start=start_date, end=end_date)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"No data found for {symbol}\")\n",
    "        return self.prepare_data(df)\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        df = df.copy()\n",
    "        # Preserve original closing price for portfolio calculations\n",
    "        df['CloseOrig'] = df['Close'].astype(float)\n",
    "        \n",
    "        # Convert to one-dimensional float64 numpy arrays\n",
    "        close_prices = np.array(df['Close'], dtype=np.float64).flatten()\n",
    "        volume = np.array(df['Volume'], dtype=np.float64).flatten()\n",
    "        \n",
    "        # Compute technical indicators\n",
    "        df['Returns'] = df['Close'].pct_change()\n",
    "        df['SMA_20'] = talib.SMA(close_prices, timeperiod=20)\n",
    "        df['EMA_20'] = talib.EMA(close_prices, timeperiod=20)\n",
    "        df['RSI'] = talib.RSI(close_prices, timeperiod=14)\n",
    "        macd, signal, _ = talib.MACD(close_prices)\n",
    "        df['MACD'] = macd\n",
    "        df['MACD_signal'] = signal\n",
    "        bb_upper, bb_middle, bb_lower = talib.BBANDS(close_prices)\n",
    "        df['BB_upper'] = bb_upper\n",
    "        df['BB_middle'] = bb_middle\n",
    "        df['BB_lower'] = bb_lower\n",
    "        df['OBV'] = talib.OBV(close_prices, volume)\n",
    "        df['MOM'] = talib.MOM(close_prices, timeperiod=14)\n",
    "        \n",
    "        # Define state representation features (do not scale CloseOrig)\n",
    "        features = ['Close', 'Returns', 'SMA_20', 'EMA_20', 'RSI', \n",
    "                    'MACD', 'MACD_signal', 'BB_upper', 'BB_middle',\n",
    "                    'BB_lower', 'OBV', 'MOM']\n",
    "        # Fill missing values and normalize\n",
    "        df[features] = df[features].fillna(method='ffill').fillna(method='bfill')\n",
    "        df[features] = self.scaler.fit_transform(df[features])\n",
    "        df = df.dropna()\n",
    "        print(f\"Data shape after preparation: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "# -------------------------\n",
    "# Trading Environment for Testing\n",
    "# -------------------------\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=10000):\n",
    "        if data.empty:\n",
    "            raise ValueError(\"Data cannot be empty\")\n",
    "        # Reset index for sequential simulation\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        # Must match training state features\n",
    "        self.features = ['Close', 'Returns', 'SMA_20', 'EMA_20', 'RSI', \n",
    "                         'MACD', 'MACD_signal', 'BB_upper', 'BB_middle',\n",
    "                         'BB_lower', 'OBV', 'MOM']\n",
    "        if 'CloseOrig' not in self.data.columns:\n",
    "            raise ValueError(\"Missing 'CloseOrig' column.\")\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0.0  # Number of shares held\n",
    "        self.current_step = 0\n",
    "        self.portfolio_value_history = [self.initial_balance]\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        return self.data.iloc[self.current_step][self.features].values\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Retrieve the unscaled current price for trading decisions.\n",
    "        current_price = float(self.data.iloc[self.current_step]['CloseOrig'])\n",
    "        prev_value = self.portfolio_value_history[-1]\n",
    "        \n",
    "        # Action mapping (5 discrete actions):\n",
    "        # 0: Sell all, 1: Sell half, 2: Hold\n",
    "        # 3: Buy with 50% available cash, 4: Buy with full available cash.\n",
    "        if current_price > 0:\n",
    "            if action == 0 and self.position > 0:\n",
    "                self.balance += current_price * self.position\n",
    "                self.position = 0.0\n",
    "            elif action == 1 and self.position > 0:\n",
    "                shares_to_sell = self.position * 0.5\n",
    "                self.balance += current_price * shares_to_sell\n",
    "                self.position -= shares_to_sell\n",
    "            elif action == 3 and self.balance > 0:\n",
    "                cash_to_use = self.balance * 0.5\n",
    "                shares_to_buy = cash_to_use / current_price\n",
    "                self.position += shares_to_buy\n",
    "                self.balance -= cash_to_use\n",
    "            elif action == 4 and self.balance > 0:\n",
    "                shares_to_buy = self.balance / current_price\n",
    "                self.position += shares_to_buy\n",
    "                self.balance = 0\n",
    "                \n",
    "        new_value = self.balance + (self.position * current_price)\n",
    "        self.portfolio_value_history.append(new_value)\n",
    "        \n",
    "        reward = (new_value - prev_value) / prev_value if prev_value > 0 else 0\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = (self.current_step >= len(self.data) - 1)\n",
    "        next_state = self._get_state() if not done else np.zeros(len(self.features))\n",
    "        return next_state, reward, done\n",
    "\n",
    "# -------------------------\n",
    "# DQNAgent: Defines the Network & Action Selection (Greedy for Testing)\n",
    "# -------------------------\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        # Set epsilon=0 for testing (greedy policy)\n",
    "        self.epsilon = 0.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 32\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action_values = self(state_tensor)\n",
    "            return torch.argmax(action_values).item()\n",
    "\n",
    "# -------------------------\n",
    "# Test Simulation Function: Returns Portfolio History, Actions, and Signals\n",
    "# -------------------------\n",
    "def test_agent(agent, env):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    actions_taken = []\n",
    "    signals = []  # record \"buy\", \"sell\", or \"hold\" for each step.\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        actions_taken.append(action)\n",
    "        # Define buy signals for actions 3 & 4, sell signals for actions 0 & 1.\n",
    "        if action in [3, 4]:\n",
    "            signals.append(\"buy\")\n",
    "        elif action in [0, 1]:\n",
    "            signals.append(\"sell\")\n",
    "        else:\n",
    "            signals.append(\"hold\")\n",
    "        state, reward, done = env.step(action)\n",
    "    return env.portfolio_value_history, actions_taken, signals\n",
    "\n",
    "# -------------------------\n",
    "# Plotting Functions\n",
    "# -------------------------\n",
    "def plot_portfolio(portfolio_values, signals, title):\n",
    "    # Prepare markers for buy and sell signals along the portfolio evolution\n",
    "    buy_indices = [i for i, s in enumerate(signals) if s == \"buy\"]\n",
    "    sell_indices = [i for i, s in enumerate(signals) if s == \"sell\"]\n",
    "    buy_values = [portfolio_values[i] for i in buy_indices]\n",
    "    sell_values = [portfolio_values[i] for i in sell_indices]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(portfolio_values))),\n",
    "        y=portfolio_values,\n",
    "        mode='lines+markers',\n",
    "        name='Portfolio Value'\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=buy_indices,\n",
    "        y=buy_values,\n",
    "        mode='markers',\n",
    "        name='Buy Signal',\n",
    "        marker=dict(color='green', size=10, symbol='triangle-up')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sell_indices,\n",
    "        y=sell_values,\n",
    "        mode='markers',\n",
    "        name='Sell Signal',\n",
    "        marker=dict(color='red', size=10, symbol='triangle-down')\n",
    "    ))\n",
    "    fig.update_layout(title=title,\n",
    "                      xaxis_title='Trading Steps',\n",
    "                      yaxis_title='Portfolio Value ($)')\n",
    "    fig.show()\n",
    "\n",
    "def plot_price_signals(test_data, signals, title):\n",
    "    # Plot price (CloseOrig) with buy and sell markers.\n",
    "    # Use the environment's reset data (i.e. a sequential index)\n",
    "    prices = test_data['CloseOrig'].values\n",
    "    steps = list(range(len(prices)))\n",
    "    \n",
    "    buy_indices = [i for i, s in enumerate(signals) if s == \"buy\"]\n",
    "    sell_indices = [i for i, s in enumerate(signals) if s == \"sell\"]\n",
    "    buy_prices = [prices[i] for i in buy_indices]\n",
    "    sell_prices = [prices[i] for i in sell_indices]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=steps,\n",
    "        y=prices,\n",
    "        mode='lines',\n",
    "        name='Price'\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=buy_indices,\n",
    "        y=buy_prices,\n",
    "        mode='markers',\n",
    "        name='Buy Signal',\n",
    "        marker=dict(color='green', size=10, symbol='triangle-up')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sell_indices,\n",
    "        y=sell_prices,\n",
    "        mode='markers',\n",
    "        name='Sell Signal',\n",
    "        marker=dict(color='red', size=10, symbol='triangle-down')\n",
    "    ))\n",
    "    fig.update_layout(title=title,\n",
    "                      xaxis_title='Trading Steps',\n",
    "                      yaxis_title='Price ($)')\n",
    "    fig.show()\n",
    "\n",
    "# -------------------------\n",
    "# Main Testing Routine\n",
    "# -------------------------\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Dictionary of indexes to test: name -> ticker symbol\n",
    "        indexes = {\n",
    "            \"USA_S&P500\": \"^GSPC\",        # S&P 500 Index\n",
    "            \"India_Nifty50\": \"^NSEI\",      # Nifty 50 Index\n",
    "            \"Japan_Nikkei225\": \"^N225\",    # Nikkei 225 Index\n",
    "            \"UK_FTSE100\": \"^FTSE\",         # FTSE 100 Index\n",
    "            \"France_CAC40\": \"^FCHI\"        # CAC 40 Index\n",
    "        }\n",
    "        # Testing period of one year:\n",
    "        TEST_START_DATE = '2024-01-01'\n",
    "        TEST_END_DATE   = '2025-01-01'\n",
    "        TEST_INITIAL_BALANCE = 10000\n",
    "        \n",
    "        # Load the pre-trained model.\n",
    "        state_size = 12  # Must match state dimensions used in training.\n",
    "        action_size = 5  # Five discrete actions.\n",
    "        agent = DQNAgent(state_size, action_size)\n",
    "        model_path = 'final_model.pth'\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        agent.load_state_dict(checkpoint)  # Adjust if checkpoint is nested.\n",
    "        agent.eval()\n",
    "        print(\"Pre-trained model loaded successfully.\")\n",
    "        \n",
    "        # Initialize the DataHandler.\n",
    "        data_handler = DataHandler()\n",
    "        \n",
    "        # Test the model on each index.\n",
    "        for index_name, ticker in indexes.items():\n",
    "            try:\n",
    "                print(\"\\n============================================\")\n",
    "                print(f\"Testing on {index_name} ({ticker})\")\n",
    "                test_data = data_handler.fetch_data(ticker, TEST_START_DATE, TEST_END_DATE)\n",
    "                # Initialize the TradingEnvironment using the prepared data.\n",
    "                env = TradingEnvironment(test_data, initial_balance=TEST_INITIAL_BALANCE)\n",
    "                portfolio_history, actions_taken, signals = test_agent(agent, env)\n",
    "                final_value = portfolio_history[-1]\n",
    "                return_pct = ((final_value / TEST_INITIAL_BALANCE) - 1) * 100\n",
    "                print(f\"Initial Balance: ${TEST_INITIAL_BALANCE:.2f}\")\n",
    "                print(f\"Final Portfolio Value: ${final_value:.2f}\")\n",
    "                print(f\"Cumulative Return: {return_pct:.2f}%\")\n",
    "                \n",
    "                title_portfolio = f\"{index_name} Portfolio Evolution (Return: {return_pct:.2f}%)\"\n",
    "                plot_portfolio(portfolio_history, signals, title_portfolio)\n",
    "                \n",
    "                title_price = f\"{index_name} Price with Buy/Sell Signals\"\n",
    "                # Use the original test_data (not reset) for price plotting with the same sequential order.\n",
    "                test_data_reset = test_data.reset_index(drop=True)\n",
    "                plot_price_signals(test_data_reset, signals, title_price)\n",
    "            except Exception as inner_ex:\n",
    "                print(f\"Error testing on {ticker}: {inner_ex}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
